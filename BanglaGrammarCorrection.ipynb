{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":8107496,"sourceType":"datasetVersion","datasetId":4788759}],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport warnings\n\nos.environ[\"WANDB_MODE\"] = \"dryrun\"\nwarnings.filterwarnings('ignore')","metadata":{"execution":{"iopub.status.busy":"2024-04-25T03:08:04.832480Z","iopub.execute_input":"2024-04-25T03:08:04.832864Z","iopub.status.idle":"2024-04-25T03:08:04.846871Z","shell.execute_reply.started":"2024-04-25T03:08:04.832833Z","shell.execute_reply":"2024-04-25T03:08:04.844784Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"code","source":"!pip install git+https://github.com/csebuetnlp/normalizer\n!pip install jiwer","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-25T03:08:04.849374Z","iopub.execute_input":"2024-04-25T03:08:04.849754Z","iopub.status.idle":"2024-04-25T03:08:45.326031Z","shell.execute_reply.started":"2024-04-25T03:08:04.849722Z","shell.execute_reply":"2024-04-25T03:08:45.324713Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/csebuetnlp/normalizer\n  Cloning https://github.com/csebuetnlp/normalizer to /tmp/pip-req-build-60ka3f73\n  Running command git clone --filter=blob:none --quiet https://github.com/csebuetnlp/normalizer /tmp/pip-req-build-60ka3f73\n  Resolved https://github.com/csebuetnlp/normalizer to commit d405944dde5ceeacb7c2fd3245ae2a9dea5f35c9\n  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: regex in /opt/conda/lib/python3.10/site-packages (from normalizer==0.0.1) (2023.12.25)\nCollecting emoji==1.4.2 (from normalizer==0.0.1)\n  Downloading emoji-1.4.2.tar.gz (184 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m185.0/185.0 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hCollecting ftfy==6.0.3 (from normalizer==0.0.1)\n  Downloading ftfy-6.0.3.tar.gz (64 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.2/64.2 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n\u001b[?25hRequirement already satisfied: wcwidth in /opt/conda/lib/python3.10/site-packages (from ftfy==6.0.3->normalizer==0.0.1) (0.2.13)\nBuilding wheels for collected packages: normalizer, emoji, ftfy\n  Building wheel for normalizer (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for normalizer: filename=normalizer-0.0.1-py3-none-any.whl size=6859 sha256=f4dfcdcbbba9e5ae5d0b810cb50fcd38c5981d5ea23a919a558e83cfe6b2902b\n  Stored in directory: /tmp/pip-ephem-wheel-cache-d8o8besc/wheels/2e/79/9c/cd96d490298305d51d2da11484bb2c25fd1f759a6906708282\n  Building wheel for emoji (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for emoji: filename=emoji-1.4.2-py3-none-any.whl size=186460 sha256=8c0559125ca2d7a495cd94b412dd737cb82d5ebf45f3c49fbac2ebd8a17cee45\n  Stored in directory: /root/.cache/pip/wheels/10/f0/fd/4813b1177405693e8da9cdea839f0fb64fde161380e058c827\n  Building wheel for ftfy (setup.py) ... \u001b[?25ldone\n\u001b[?25h  Created wheel for ftfy: filename=ftfy-6.0.3-py3-none-any.whl size=41929 sha256=b9bd27285cd9b54720bd89e1b817a4fdab2ca4629307f0772cd510405aa4eb66\n  Stored in directory: /root/.cache/pip/wheels/92/8e/16/c1e4d4d65685d71085e4e27b44d6ed880b0559474c9ee4ff66\nSuccessfully built normalizer emoji ftfy\nInstalling collected packages: emoji, ftfy, normalizer\n  Attempting uninstall: emoji\n    Found existing installation: emoji 2.11.0\n    Uninstalling emoji-2.11.0:\n      Successfully uninstalled emoji-2.11.0\nSuccessfully installed emoji-1.4.2 ftfy-6.0.3 normalizer-0.0.1\nCollecting jiwer\n  Downloading jiwer-3.0.3-py3-none-any.whl.metadata (2.6 kB)\nRequirement already satisfied: click<9.0.0,>=8.1.3 in /opt/conda/lib/python3.10/site-packages (from jiwer) (8.1.7)\nCollecting rapidfuzz<4,>=3 (from jiwer)\n  Downloading rapidfuzz-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\nDownloading jiwer-3.0.3-py3-none-any.whl (21 kB)\nDownloading rapidfuzz-3.8.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hInstalling collected packages: rapidfuzz, jiwer\nSuccessfully installed jiwer-3.0.3 rapidfuzz-3.8.1\n","output_type":"stream"}]},{"cell_type":"code","source":"from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, DataCollatorForSeq2Seq, Seq2SeqTrainingArguments, Seq2SeqTrainer\nimport torch\nimport numpy as np\nfrom torch.utils.data import Dataset\nfrom normalizer import normalize","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-25T03:08:45.328403Z","iopub.execute_input":"2024-04-25T03:08:45.328746Z","iopub.status.idle":"2024-04-25T03:09:04.637552Z","shell.execute_reply.started":"2024-04-25T03:08:45.328716Z","shell.execute_reply":"2024-04-25T03:09:04.636417Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stderr","text":"2024-04-25 03:08:53.604725: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-04-25 03:08:53.604831: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-04-25 03:08:53.736588: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}]},{"cell_type":"code","source":"import pandas as pd\n\ntrain_df = pd.read_csv(\"/kaggle/input/bnsecdataset/BNSECDataset/train.csv\")\ntest_df = pd.read_csv(\"/kaggle/input/bnsecdataset/BNSECDataset/test.csv\")\n\ntrain_df.iloc[0].Input, train_df.iloc[0].Target","metadata":{"execution":{"iopub.status.busy":"2024-04-25T03:09:04.638740Z","iopub.execute_input":"2024-04-25T03:09:04.639378Z","iopub.status.idle":"2024-04-25T03:09:17.683315Z","shell.execute_reply.started":"2024-04-25T03:09:04.639347Z","shell.execute_reply":"2024-04-25T03:09:17.682266Z"},"trusted":true},"execution_count":4,"outputs":[{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"('রিয়াল ভাবটে পরে তারা আদলে 1111111111 পয়েন্টে এগিয়ে ।',\n 'রিয়াল ভাবতে পারে তারা আসলে 1111111111 পয়েন্টে এগিয়ে ।')"},"metadata":{}}]},{"cell_type":"code","source":"print(train_df.isnull().sum())\nprint(train_df.duplicated().sum())","metadata":{"execution":{"iopub.status.busy":"2024-04-25T03:09:17.685585Z","iopub.execute_input":"2024-04-25T03:09:17.685929Z","iopub.status.idle":"2024-04-25T03:09:20.807767Z","shell.execute_reply.started":"2024-04-25T03:09:17.685876Z","shell.execute_reply":"2024-04-25T03:09:20.806720Z"},"trusted":true},"execution_count":5,"outputs":[{"name":"stdout","text":"Target    0\nInput     0\ndtype: int64\n1630\n","output_type":"stream"}]},{"cell_type":"code","source":"train_df.dropna(inplace=True)\ntest_df.dropna(inplace = True)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T03:09:20.809438Z","iopub.execute_input":"2024-04-25T03:09:20.809872Z","iopub.status.idle":"2024-04-25T03:09:21.259232Z","shell.execute_reply.started":"2024-04-25T03:09:20.809835Z","shell.execute_reply":"2024-04-25T03:09:21.257949Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"train_df.shape","metadata":{"execution":{"iopub.status.busy":"2024-04-25T03:09:21.260690Z","iopub.execute_input":"2024-04-25T03:09:21.261147Z","iopub.status.idle":"2024-04-25T03:09:21.269233Z","shell.execute_reply.started":"2024-04-25T03:09:21.261110Z","shell.execute_reply":"2024-04-25T03:09:21.268080Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(1356300, 2)"},"metadata":{}}]},{"cell_type":"code","source":"from tqdm import tqdm\n\nsentences_with_errors_train = [\n    normalize(erroneous_sentence) for erroneous_sentence in tqdm(train_df['Input'], desc=\"Processing training data\")\n]\ncorrected_sentences_train = [\n    normalize(target_sentence) for target_sentence in tqdm(train_df['Target'], desc=\"Processing training targets\")\n]\n\nsentences_with_errors_test = [\n    normalize(erroneous_sentence) for erroneous_sentence in tqdm(test_df['Input'], desc=\"Processing test data\")\n]\ncorrected_sentences_test = [\n    normalize(target_sentence) for target_sentence in tqdm(test_df['Target'], desc=\"Processing test targets\")\n]\n\n# len(corrected_sentences_train)\n# print(corrected_sentences_train)\n# sentences_with_errors_train[0], corrected_sentences_train[0]","metadata":{"scrolled":true,"execution":{"iopub.status.busy":"2024-04-25T03:09:21.270509Z","iopub.execute_input":"2024-04-25T03:09:21.270851Z","iopub.status.idle":"2024-04-25T03:19:26.617426Z","shell.execute_reply.started":"2024-04-25T03:09:21.270812Z","shell.execute_reply":"2024-04-25T03:19:26.616351Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stderr","text":"Processing training data: 100%|██████████| 1356300/1356300 [05:01<00:00, 4503.92it/s]\nProcessing training targets: 100%|██████████| 1356300/1356300 [05:03<00:00, 4464.95it/s]\nProcessing test data: 100%|██████████| 899/899 [00:00<00:00, 4346.65it/s]\nProcessing test targets: 100%|██████████| 899/899 [00:00<00:00, 4279.41it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"# model_checkpoint = \"google-t5/t5-base\"\nmodel_checkpoint = \"csebuetnlp/banglat5\"\nmodel = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\ntokenizer = AutoTokenizer.from_pretrained(model_checkpoint, use_fast=False)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T03:19:26.618748Z","iopub.execute_input":"2024-04-25T03:19:26.619061Z","iopub.status.idle":"2024-04-25T03:19:54.724033Z","shell.execute_reply.started":"2024-04-25T03:19:26.619036Z","shell.execute_reply":"2024-04-25T03:19:54.723181Z"},"trusted":true},"execution_count":9,"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a4853b8ea9b4e55be75b976172d63e1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"pytorch_model.bin:   0%|          | 0.00/990M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"78079d7a2d79415496dc3ab08c829d2d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.83k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6b89325018894fc9aa0efbd0705ea09b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"spiece.model:   0%|          | 0.00/1.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"841e8751680143fb839096993571fd4f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/1.79k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e356be22128345d98b81681a2fc03756"}},"metadata":{}},{"name":"stderr","text":"You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n","output_type":"stream"}]},{"cell_type":"code","source":"text = \"আমি আমার মন ঠিক করেছি। আমি একটা ট্যাটু আঁকছি।\"\ninput_encoding = tokenizer(\n    text=normalize(text), \n    return_tensors='pt', \n    max_length=512, \n    padding='max_length', \n    truncation=True\n)\n\n# Generate corrected sentence\ngenerated_ids = model.generate(\n    input_encoding[\"input_ids\"]\n)\n\n# Decode the generated sequence into text\ngenerated_text = tokenizer.decode(\n    generated_ids[0], \n    skip_special_tokens=True\n)\nprint(\"generated sentence:\", generated_text)","metadata":{"execution":{"iopub.status.busy":"2024-04-25T03:21:34.870048Z","iopub.execute_input":"2024-04-25T03:21:34.870506Z","iopub.status.idle":"2024-04-25T03:21:37.254392Z","shell.execute_reply.started":"2024-04-25T03:21:34.870476Z","shell.execute_reply":"2024-04-25T03:21:37.252255Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"generated sentence: আমি আমার মন ঠিক করেছি। আমি একটা ট্যাটু আঁকছি। আমি আমার মন ঠিক করেছি।\n","output_type":"stream"}]},{"cell_type":"code","source":"class GrammarCorrectionDataset(Dataset):\n    def __init__(self, sentences_with_errors, corrected_sentences, tokenizer, split_type='train'):\n        self.sentences_with_errors = sentences_with_errors\n        self.corrected_sentences = corrected_sentences\n        self.tokenizer = tokenizer\n        self.split_type = split_type\n        self.vocab_size = tokenizer.vocab_size\n        self.max_sequence_length = 128  # Adjust as needed\n\n    def __len__(self):\n        return len(self.sentences_with_errors)\n\n    def __getitem__(self, idx):\n        sentence_with_error = self.sentences_with_errors[idx]\n        corrected_sentence = self.corrected_sentences[idx]\n\n        input_encoding = self.tokenizer(\n            sentence_with_error, \n            return_tensors='pt', \n            max_length=self.max_sequence_length, \n            padding='max_length', \n            truncation=True\n        )\n        label_encoding = self.tokenizer(\n            corrected_sentence, \n            return_tensors='pt', \n            max_length=self.max_sequence_length, \n            padding='max_length', \n            truncation=True\n        )\n\n        return {\n            \"input_ids\": input_encoding[\"input_ids\"].squeeze(), \n            \"labels\": label_encoding[\"input_ids\"].squeeze()\n        }","metadata":{"execution":{"iopub.status.busy":"2024-04-25T03:21:37.256446Z","iopub.execute_input":"2024-04-25T03:21:37.256782Z","iopub.status.idle":"2024-04-25T03:21:37.267486Z","shell.execute_reply.started":"2024-04-25T03:21:37.256755Z","shell.execute_reply":"2024-04-25T03:21:37.266488Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"batch_size = 8  # Adjust as needed\nmodel_name = model_checkpoint.split(\"/\")[-1]\nargs = Seq2SeqTrainingArguments(\n    output_dir=f\"{model_name}-finetuned-grammar-correction\",\n    evaluation_strategy=\"epoch\",\n    learning_rate=1e-5,\n    per_device_train_batch_size=batch_size,\n    per_device_eval_batch_size=batch_size,\n    weight_decay=0.01,\n    save_total_limit=1,\n    num_train_epochs=1,  # Adjust as needed\n    predict_with_generate=True,\n    skip_memory_metrics=False,\n    push_to_hub=False,\n)\n\n# prepare dataset\ntrain_dataset = GrammarCorrectionDataset(\n    sentences_with_errors_train,\n    corrected_sentences_train,\n    tokenizer,\n    split_type='train'\n)\ntest_dataset = GrammarCorrectionDataset(\n    sentences_with_errors_test, \n    corrected_sentences_test, \n    tokenizer, \n    split_type='test'\n)\n\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer=tokenizer, \n    model=model\n)\n\ntrainer = Seq2SeqTrainer(\n    model=model,\n    args=args,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n)\n\ntrainer.train()","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-04-25T03:21:37.268863Z","iopub.execute_input":"2024-04-25T03:21:37.269811Z"},"trusted":true},"execution_count":null,"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.16.6"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"W&B syncing is set to <code>`offline`<code> in this directory.  <br/>Run <code>`wandb online`<code> or set <code>WANDB_MODE=online<code> to enable cloud syncing."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='95' max='169538' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [    95/169538 00:33 < 17:04:51, 2.76 it/s, Epoch 0.00/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n  </tbody>\n</table><p>"},"metadata":{}}]},{"cell_type":"code","source":"# # Assuming `model` and `tokenizer` are already defined and trained\n# sentence_with_error = \"আমি ভ্যাট খাই\"  # Example sentence with error\n# input_encoding = tokenizer(normalize(sentence_with_error), return_tensors='pt', max_length=128, padding='max_length', truncation=True)\n# generated_ids = model.generate(input_encoding[\"input_ids\"])\n# corrected_sentence = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n# print(\"Corrected sentence:\", corrected_sentence)","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_output_dir = \"./trained_grammar_correction_model\"\nmodel.save_pretrained(model_output_dir)\ntokenizer.save_pretrained(model_output_dir)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n\n\nloaded_model = AutoModelForSeq2SeqLM.from_pretrained(model_output_dir)\nloaded_tokenizer = AutoTokenizer.from_pretrained(model_output_dir, use_fast=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# sentence_with_error = \"রিয়াল ভাবটে পরে তারা আদলে 1111111111 পয়েন্টে এগিয়ে ।\"\nsentence_with_error = \"আমি ভ্যাট খাই\"\ninput_encoding = loaded_tokenizer(\n    text=normalize(sentence_with_error), \n    return_tensors='pt', \n    max_length=128, \n    padding='max_length', \n    truncation=True\n)\n\n# Generate corrected sentence\ngenerated_ids = loaded_model.generate(\n    input_encoding[\"input_ids\"]\n)\n\n# Decode the generated sequence into text\ncorrected_sentence = loaded_tokenizer.decode(\n    generated_ids[0], \n    skip_special_tokens=True\n)\nprint(\"Corrected sentence:\", corrected_sentence)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}